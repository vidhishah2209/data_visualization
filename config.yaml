# General settings
general:
  project_name: "Automated Data Science Workflow"
  random_seed: 42
  n_jobs: -1  # Use all available cores
  use_gpu: true
  verbose: true

# Data settings
data:
  input_path: "data/raw/creditCardFraud_28011964_120214 (2).csv"
  output_path: "data/processed/"
  test_size: 0.2
  validation_size: 0.1
  stratify: true
  file_formats:
    - csv
    - json
    - excel
    - parquet
  target_column: "default payment next month"
  random_state: 42

# Preprocessing settings
preprocessing:
  auto_detect_types: true
  handle_missing_values: true
  missing_value_strategy: "mean"  # Options: mean, median, mode
  handle_outliers: true
  outlier_detection:
    method: "isolation_forest"
    contamination: 0.1
  scaling: true
  scaling_method: "auto"  # Will be determined by Gemini
  categorical_encoding: "auto"  # Options: onehot, label, target, frequency, auto
  datetime_features: true
  text_cleaning: true
  feature_selection:
    method: "variance_threshold"
  dimensionality_reduction:
    method: "pca"

# Feature engineering settings
feature_engineering:
  use_llm: false
  llm_model: "models/gemini-1.5-pro-latest"
  embedding_model: "models/embedding-001"
  feature_selection: true
  feature_selection_method: "recursive"  # Options: variance, correlation, mutual_info, recursive, lasso
  dimensionality_reduction: "auto"  # Options: pca, tsne, umap, auto, none
  polynomial_features:
    enabled: false
    degree: 2
  interaction_features: false
  clustering_features:
    enabled: false
    n_clusters: 3
  text_embeddings:
    enabled: false
    model: "sentence-transformers/all-MiniLM-L6-v2"
  max_features: 100
  time_features: false  # Set to true if dealing with time series data

# Model settings
model:
  task: "auto"  # Options: classification, regression, clustering, auto
  auto_select: true
  models_to_try:
    - "random_forest"
    - "gradient_boosting"
    - "logistic_regression"
  hyperparameter_tuning:
    enabled: true
    method: "grid_search"
    cv: 5
  n_trials: 100
  cv_folds: 5
  early_stopping: true
  ensemble: true
  ensemble_method: "stacking"  # Options: voting, bagging, stacking, boosting
  metric: "auto"  # Will be set based on task

# Explainability settings
explainability:
  generate_shap: true
  generate_lime: true
  feature_importance: true
  partial_dependence: true
  llm_explanations: true
  explanation_format: "markdown"  # Options: markdown, html, text
  shap_values: true
  lime_explanations: true
  output_dir: "results/explanations"

# Results and reporting
results:
  save_model: true
  save_predictions: true
  generate_report: true
  generate_visualizations: true
  visualization_library: "plotly"  # Options: matplotlib, seaborn, plotly
  generate_presentation: true

# LLM API settings
llm_api:
  gemini_api_key: "AIzaSyBpDLKzRtdE3d2_dmljIqfaqrKUdGJVnT8"
  use_llm: false  # Disable LLM usage
  max_tokens: 1000
  temperature: 0.8
  provider: "gemini"
  api_key: "AIzaSyBpDLKzRtdE3d2_dmljIqfaqrKUdGJVnT8"  # Add your API key here

# Visualization settings
visualization:
  style: "default"
  color_palette: "viridis"
  figure_size: [12, 8]
  dpi: 300
  save_format: "html"  # Options: "html", "png", "pdf"
  interactive: true
  plot_types:
    - "distribution"
    - "correlation"
    - "scatter"
    - "box"
    - "3d_pca"
    - "feature_importance"
  enabled: true
  plots:
    - "distribution"
    - "correlation"
    - "scatter"
    - "box"
    - "pca"
    - "feature_importance"
  output_dir: "results/visualizations"

# Evaluation metrics
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
    - "confusion_matrix"
  cross_validation:
    enabled: true
    folds: 5
    shuffle: true

# Output settings
output:
  save_predictions: true
  save_models: true
  save_visualizations: true
  save_metrics: true
  save_feature_importance: true
  results_dir: "results"
  model_dir: "results/models"
  visualization_dir: "results/visualizations"
  metrics_dir: "results/metrics"
  plots_dir: "results/plots"
  output_dir: "results"
  log_dir: "logs"

# Logging settings
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/pipeline.log" 